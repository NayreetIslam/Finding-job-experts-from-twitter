{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'oKebnsv4ofhF1AwmEfkV9CGbG'\n",
    "consumer_secret = 'hICgy8pgfx7omYz0vRExanOrl0ejOfJkPVDMVrNx6ZWb2vkCWw'\n",
    "access_token = '2974494974-9nu3LxdZ3CTNtvVcZnPeKNPXrst7wHEoVRm5Xu1'\n",
    "access_secret = 'CF8FLLntfOwNPTBkAgX0TiwHwWs52NVIiny12ZOoyY3k2'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_expert_tweets = []\n",
    "def get_all_tweets(screen_name):\n",
    "    with open('C:\\\\Users\\\\100631155\\\\Recruiter\\\\%s_tweets.csv' % screen_name, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for tweet in tweepy.Cursor(api.user_timeline,id=screen_name).items():\n",
    "            userid = tweet.id_str\n",
    "            text=tweet.text.encode(\"utf-8\")\n",
    "            all_expert_tweets.append(text)\n",
    "            time = tweet.created_at\n",
    "            writer.writerow([userid,time,text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a user search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = api.search('job recruiter')\n",
    "for user in results:\n",
    "    print(user.id, user.author.screen_name)\n",
    "    get_all_tweets(user.author.screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "import numpy as np\n",
    "import twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets():\n",
    "    tweets = []\n",
    "    max_id = None\n",
    "    for _ in range(100):\n",
    "        tweets.extend(list(api.GetUserTimeline(screen_name='Hire_Colorado',\n",
    "                                               max_id=max_id,\n",
    "                                               count=200,\n",
    "                                               include_rts=False,\n",
    "                                               exclude_replies=True)))\n",
    "        max_id = tweets[-1].id\n",
    "    return [tweet.text for tweet in tweets]\n",
    "\n",
    "np.save(file='meni_tweets.npy', arr=get_tweets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS_LENGTH = None\n",
    "\n",
    "def get_corpus(verbose=0):\n",
    "    tweets = np.load('meni_tweets.npy')\n",
    "    tweets = [t for t in tweets if 'http' not in t]\n",
    "    tweets = [t for t in tweets if '&gt' not in t]\n",
    "    corpus = u' '.join(tweets)\n",
    "    global CORPUS_LENGTH\n",
    "    CORPUS_LENGTH = len(corpus)\n",
    "    if verbose:\n",
    "        print('Corpus Length:', CORPUS_LENGTH)\n",
    "    return corpus\n",
    "\n",
    "corpus = get_corpus(verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import word2vec\n",
    "from sklearn.utils import shuffle\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert label to numeric vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_convert(data):\n",
    "    enc = LabelEncoder()\n",
    "    nLabels = len(set(enc.fit(data).classes_))\n",
    "    data = enc.transform(data)\n",
    "    return np.eye(nLabels)[data].astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_line(line):    \n",
    "    if re.search(\"http[\\S]+\", line):\n",
    "        line = re.sub(\"http[\\S]+\",\"<url>\", line)        \n",
    "    line = re.sub(\"\\Wamp+\",\"and\", line)    \n",
    "    if re.search(\"[A-Za-z]\\W\\w+\",line):\n",
    "        line  = re.sub(\"[^A-Za-z\\d\\s\\'\\˘\\’\\.\\,]+\",\" \", line)\n",
    "    line = line.lower()   \n",
    "    cleanLine = word_tokenize(line)\n",
    "    return cleanLine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Single string for word to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_single_string(dataset):\n",
    "    q = \" \"\n",
    "    stringData = []\n",
    "    for row in dataset:\n",
    "        line = q.join(row)\n",
    "        stringData.append(line)\n",
    "    string = q.join(stringData) \n",
    "    return {\"single_string\" : string, \"lines\" : stringData }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(x1, y1, x2, y2, label1 = None, label2 = None, title = None, x_label = None, y_label = None):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax1 = fig.add_subplot(1,1,1)\n",
    "    ax1.plot(x1, y1, color='navy', label = label1)\n",
    "    ax1.tick_params(bottom='off',top='off',left='off',right='off')\n",
    "    ax1.spines[\"right\"].set_visible(False)\n",
    "    ax1.spines[\"left\"].set_visible(False)\n",
    "    ax1.spines[\"top\"].set_visible(False)\n",
    "    ax1.spines[\"bottom\"].set_visible(False)\n",
    "    ax1.set(title= title, ylabel= y_label, xlabel = x_label)\n",
    "    ax1.plot(x2, y2,color= 'green', label = label2)\n",
    "    ax1.legend(loc= 'upper center', bbox_to_anchor=(0.5, -0.1),  shadow=True, ncol=2)\n",
    "    ax1.grid()\n",
    "    plt.savefig(\"{}.png\".format(title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('train.csv','r', encoding = 'utf8')\n",
    "trainData = list(csv.reader(f))[1:]\n",
    "f = open('test.csv','r', encoding = 'utf8')\n",
    "testData = list(csv.reader(f)) [1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Shuffle training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get target and raw input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainTarget, trainSentences = zip(*trainData) #trainIds\n",
    "#testIds, testTarget, testSentences = zip(*testData) \n",
    "\n",
    "noRows = len(trainSentences)\n",
    "trainTarget = label_convert(trainTarget)\n",
    "#testTarget = label_convert(testTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanTrain = [process_line(row) for row in trainSentences]\n",
    "#cleanTest = [process_line(row) for row in testSentences]\n",
    "\n",
    "whole = cleanTrain + cleanTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train word to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(whole, size  = 30, window = 10, min_count=1, iter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxLength = 35 #max(trainLen)\n",
    "\n",
    "trainLen = [min(len(each), maxLength) for each in cleanTrain]\n",
    "testLen = [min(len(each), maxLength) for each in cleanTest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_count = 0\n",
    "trainNumeric = np.zeros((len(cleanTrain), maxLength), dtype='int32')\n",
    "\n",
    "for row in cleanTrain:\n",
    "    index_count = 0\n",
    "    for each in row:\n",
    "        try:\n",
    "            trainNumeric[row_count][index_count] = model.wv.vocab[each].index\n",
    "        except:\n",
    "            trainNumeric[row_count][index_count] = 0\n",
    "        index_count += 1\n",
    "        if index_count >= maxLength:\n",
    "            print(\"Done with the Row: \", row_count)\n",
    "            break       \n",
    "    row_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_count = 0\n",
    "testNumeric = np.zeros((len(cleanTest), maxLength), dtype='int32')\n",
    "\n",
    "for row in cleanTest:\n",
    "    index_count = 0\n",
    "    for each in row:\n",
    "        try:\n",
    "            testNumeric[row_count][index_count] = model.wv.vocab[each].index\n",
    "        except:\n",
    "            testNumeric[row_count][index_count] = 0\n",
    "        index_count += 1\n",
    "        if index_count >= maxLength:\n",
    "            print(\"Done with the Row: \", row_count)\n",
    "            break       \n",
    "    row_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddingMatrix = np.zeros((len(model.wv.vocab), 30), dtype=\"float32\")\n",
    "for i in range(len(model.wv.vocab)):\n",
    "    embeddingVector = model.wv[model.wv.index2word[i]]\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model 1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ValidIndexStart = 4199\n",
    "\n",
    "batchSize = 100\n",
    "lstmUnits = 35\n",
    "numClasses = 2\n",
    "\n",
    "epochs = 15\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_data = tf.placeholder(tf.int32, [None, maxLength])\n",
    "\n",
    "input_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "labels = tf.placeholder(tf.int32, [None, numClasses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Weight & bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W =tf.Variable(tf.random_normal([lstmUnits, numClasses]))\n",
    "b = tf.Variable(tf.random_normal([numClasses]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = tf.nn.embedding_lookup(embeddingMatrix,input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.LSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.MultiRNNCell([lstmCell])\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.5)\n",
    "\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32, sequence_length=input_length)\n",
    "\n",
    "word_index = tf.reshape(input_length, [tf.shape(value)[0]])-1\n",
    "\n",
    "sentence_index = tf.range(0, tf.shape(input_data)[0]) * (tf.shape(input_data)[1])\n",
    "                      \n",
    "index = sentence_index + word_index\n",
    "\n",
    "flat = tf.reshape(value, [-1, tf.shape(value)[2]])\n",
    "        \n",
    "last = tf.gather(flat, index)\n",
    "\n",
    "out_layer = tf.add(tf.matmul(last,W), b)\n",
    "\n",
    "prediction = tf.nn.softmax(out_layer)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_layer,\n",
    "                                                              labels=labels))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "valLoss1 = [] #results for validation loss\n",
    "trainLoss1 = [] #train loss\n",
    "valAcc1 = []\n",
    "trainAcc1 = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    #train model\n",
    "    ValNumeric, ValLen, ValTarget = trainNumeric[ValidIndexStart+1: ], trainLen[ValidIndexStart+1: ], trainTarget[ValidIndexStart+1: ]  \n",
    "    trainNumeric_, trainLen_, trainTarget_ = trainNumeric[:ValidIndexStart], trainLen[:ValidIndexStart], trainTarget[:ValidIndexStart]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        nBatch = int( trainNumeric.shape[0] / batchSize )\n",
    "        trainNumeric_, trainLen_, trainTarget_ = shuffle(trainNumeric_, trainLen_, trainTarget_)\n",
    "        trainBatchesSentenceSplit = np.array_split(trainNumeric_, nBatch)\n",
    "        trainBatchesLenSplit = np.array_split(trainLen_, nBatch)\n",
    "        trainBatchesLabelSplit = np.array_split(trainTarget_, nBatch)\n",
    "        \n",
    "        for i in range(nBatch):\n",
    "            batch = trainBatchesSentenceSplit[i]\n",
    "            length = trainBatchesLenSplit[i]\n",
    "            target = trainBatchesLabelSplit[i]\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _= sess.run([optimizer], feed_dict={input_data: batch,\n",
    "                                                  input_length: length,\n",
    "                                                  labels: target\n",
    "                                                  })\n",
    "        print(\"Epoch Done:{}\".format(epoch))\n",
    "        # Display logs per epoch\n",
    "        if epoch % display_step == 0:\n",
    "            trainLoss, trainAcc = sess.run([loss, accuracy], feed_dict = {input_data:trainNumeric_,\n",
    "                                                                          input_length:trainLen_, \n",
    "                                                                          labels:trainTarget_         \n",
    "                                                                           })\n",
    "            print(\"-\"*60)\n",
    "            print(\"Epoch: {}\\nTrain Batch:\\nLoss:{:0.4f}\\n Accuracy: {:0.3f}\\n\".format(epoch,trainLoss,trainAcc))\n",
    "            \n",
    "            # Validate model\n",
    "            ValLoss, ValAcc = sess.run([loss, accuracy],feed_dict = {input_data: ValNumeric,\n",
    "                                                                     input_length: ValLen,                               \n",
    "                                                                     labels: ValTarget \n",
    "                                                                    })\n",
    "            print(\"Validation Batch:\\n Loss:{:0.3f}\\n Accuracy:{:0.3f}\".format(ValLoss,ValAcc))\n",
    "            \n",
    "            trainLoss1.append([epoch,trainLoss])\n",
    "            trainAcc1.append([epoch,trainAcc])\n",
    "            valLoss1.append([epoch,ValLoss])\n",
    "            valAcc1.append([epoch,ValAcc])                \n",
    "                \n",
    "    print(\"Optimization Finished!\")\n",
    "    # Test model\n",
    "    testPrediction, testAcc, testLoss = sess.run([prediction, accuracy, loss], feed_dict = {input_data: testNumeric, input_length: testLen, labels: testTarget} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1, y1 = zip(*valLoss1)\n",
    "x2, y2 = zip(*trainLoss1)\n",
    "x3, y3 = zip(*valAcc1)\n",
    "x4, y4 = zip(*trainAcc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "plt.plot(x1, y1, x2, y2, label1 = \"Validation Loss\", label2 = \"Training Loss\", title = \"Model 1 Loss\",y_label = \"Loss\", x_label = \"Epoch\")\n",
    "f.savefig(\"Loss\", bbox_inches='tight')\n",
    "plot(x3, y3, x4, y4, label1 = \"Validation Accuracy\", label2 = \"Training Accuracy\", title = \"Model 1 Accuracy\",y_label = \"Accuracy\", x_label = \"Epoch\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = \"fp.1\"\n",
    "predictions_test_1 = [row for row in testPrediction]\n",
    "        \n",
    "predictions_test_1_id = []        \n",
    "for i, row in enumerate(predictions_test_1):\n",
    "    row = row.tolist()\n",
    "    row.insert(0,i)\n",
    "    predictions_test_1_id.append(row)\n",
    "predictions_test_1_id.insert(0, [\"id\", \"experts\", \"notExperts\"])\n",
    "\n",
    "with open(name + \".csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(predictions_test_1_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as net\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "twitter_network = [ line.strip().split('\\t') for line in file('twitter_network.csv') ]\n",
    "\n",
    "o = net.DiGraph()\n",
    "hfollowers = defaultdict(lambda: 0)\n",
    "for (twitter_user, followed_by, followers) in twitter_network:\n",
    "    o.add_edge(twitter_user, followed_by, followers=int(followers))\n",
    "    hfollowers[twitter_user] = int(followers)\n",
    "\n",
    "SEED = 'jasonrmartin'\n",
    "\n",
    "# centre around the SEED node and set radius of graph\n",
    "g = net.DiGraph(net.ego_graph(o, SEED, radius=4))\n",
    "\n",
    "def trim_degrees_ted(g, degree=1, ted_degree=1):\n",
    "    g2 = g.copy()\n",
    "    d = net.degree(g2)\n",
    "    for n in g2.nodes():\n",
    "        if n == SEED: continue # don't prune the SEED node\n",
    "        if d[n] <= degree and not n.lower().startswith('ted'):\n",
    "            g2.remove_node(n)\n",
    "        elif n.lower().startswith('ted') and d[n] <= ted_degree:\n",
    "            g2.remove_node(n)\n",
    "    return g2\n",
    "\n",
    "def trim_edges_ted(g, weight=1, ted_weight=10):\n",
    "    g2 = net.DiGraph()\n",
    "    for f, to, edata in g.edges_iter(data=True):\n",
    "        if f == SEED or to == SEED: # keep edges that link to the SEED node\n",
    "            g2.add_edge(f, to, edata)\n",
    "        elif f.lower().startswith('ted') or to.lower().startswith('ted'):\n",
    "            if edata['followers'] >= ted_weight:\n",
    "                g2.add_edge(f, to, edata)\n",
    "        elif edata['followers'] >= weight:\n",
    "            g2.add_edge(f, to, edata)\n",
    "    return g2\n",
    "\n",
    "print(len(g))\n",
    "core = trim_degrees_ted(g, degree=235, ted_degree=1)\n",
    "print('core after node pruning: ', len(core))\n",
    "core = trim_edges_ted(core, weight=250000, ted_weight=35000)\n",
    "print('core after edge pruning: ', len(core))\n",
    "\n",
    "nodeset_types = { 'TED': lambda s: s.lower().startswith('ted'), 'Not TED': lambda s: not s.lower().startswith('ted') }\n",
    "\n",
    "nodesets = defaultdict(list)\n",
    "\n",
    "for nodeset_typename, nodeset_test in nodeset_types.iteritems():\n",
    "    nodesets[nodeset_typename] = [ n for n in core.nodes_iter() if nodeset_test(n) ]\n",
    "\n",
    "pos = net.spring_layout(core) # compute layout\n",
    "\n",
    "colours = ['red','green']\n",
    "colourmap = {}\n",
    "\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.axis('off')\n",
    "\n",
    "# draw nodes\n",
    "i = 0\n",
    "alphas = {'TED': 0.6, 'Not TED': 0.4}\n",
    "for k in nodesets.keys():\n",
    "    ns = [ math.log10(hfollowers[n]+1) * 80 for n in nodesets[k] ]\n",
    "    print(k, len(ns))\n",
    "    net.draw_networkx_nodes(core, pos, nodelist=nodesets[k], node_size=ns, node_color=colours[i], alpha=alphas[k])\n",
    "    colourmap[k] = colours[i]\n",
    "    i += 1\n",
    "print('colourmap: ', colourmap)\n",
    "\n",
    "# draw edges\n",
    "net.draw_networkx_edges(core, pos, width=0.5, alpha=0.5)\n",
    "\n",
    "# draw labels\n",
    "alphas = { 'TED': 1.0, 'Not TED': 0.5}\n",
    "for k in nodesets.keys():\n",
    "    for n in nodesets[k]:\n",
    "        x, y = pos[n]\n",
    "        plt.text(x, y+0.02, s=n, alpha=alphas[k], horizontalalignment='center', fontsize=9)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
